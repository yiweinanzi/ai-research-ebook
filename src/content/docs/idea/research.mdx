---
title: 调研方法
description: 系统化的文献调研方法，从"看论文"到"产出空白"的完整流程
sidebar:
  order: 1
---

## 1. 调研目标

调研的目标不是"看了很多论文"，而是要产出**能直接导向可写论文的结果**：

1. 该方向到底在解决什么问题
2. 大家默认的设定/评测是什么
3. 现在的瓶颈在哪里
4. 哪些空白值得写
5. 你的 idea 能否形成可验证的 claim

## 2. 工具选择与分工

### 2.1 主力调研工具

| 工具 | 适用场景 |
|------|---------|
| **ChatGPT 5.2 Pro** | 体系化综述、把一堆材料整理成"研究地图"、提炼问题定义与评价体系 |
| **Manus / GPT 5.2 Thinking** | 抓漏洞（这篇论文到底新在哪里？有没有偷换设定？） |
| **Gemini 3 Pro Deep Research** | 补全资料、扩展相关工作、找被忽略的假设 |

### 2.2 推荐分工

- **Pro**: 搭主框架（问题→方法→实验→结论）+ 产出"标准化笔记"
- **Thinking**: 抓漏洞（创新点、评价公平性、潜在问题）
- **Deep Research**: 补引用与证据（相关工作漏了谁？还有哪些强 baseline？）

## 3. 调研的产物清单

建议把调研结果固定成 6 份输出：

1. **Related Work Map（研究地图）**
   - 按"问题子类/数据模态/方法范式/训练范式/评测范式"分层

2. **Benchmark & Metric Sheet（评测与指标表）**
   - 主流数据集/指标/协议/常见坑

3. **Assumption Matrix（隐含假设矩阵）**
   - 每篇论文默认假设是什么？（资源、标注、算力、可访问信息等）

4. **Failure Taxonomy（失败模式分类）**
   - 方法为什么失败？失败发生在什么条件下？

5. **Gap List（研究空白清单）**
   - 空白不是"没人做"，而是"没人把它做成可验证主张 + 可复现实验"

6. **Claim Candidate Pool（候选主张池）**
   - 每个 claim 必须能写成一句话，并且对应至少一个主实验 + 一个消融

## 4. 调研流程

### 4.1 定义边界 (Scope)

- 你要解决的"核心问题"一句话是什么？
- 哪些工作算"同类问题"，哪些只是"相关但不同问题"？

### 4.2 建语料库 (Corpus)

- 每篇至少记录：任务定义、输入输出、数据集、指标、训练资源、关键 novelty、关键 baseline

### 4.3 标准化阅读

不要自由笔记，要"字段化"：

- **Problem**: 解决什么，设定是什么？
- **Key Idea**: 一句话新意是什么？
- **Mechanism**: 为什么有效（作者的因果叙事）？
- **Evidence**: 哪些实验真正在支撑 claim？
- **Baselines**: 对比是否公平？
- **Weakness**: 最大漏洞是什么？
- **Reproduce**: 复现难点在哪里？

### 4.4 对齐与冲突检测

多模型调研结果可能冲突，冲突往往就是"机会点"：

- 该方向究竟默认设定是什么？
- 某些工作是不是在换设定？
- 指标是不是被 hack 了？
- 某些强 baseline 是否被刻意忽略？

### 4.5 产出 gap → 变成可写 idea

每个 gap 最终要落到**可写论文的形式**：

- Claim（主张）
- Setup（设定）
- Method（方法）
- Experiment（实验）
- Expected Outcome（预期）
- Risk（最大风险与备选方案）

## 5. 文献检索工具 (2025最新)

| 工具 | 核心功能 | 适用场景 | 链接 |
|------|---------|---------|------|
| **Elicit** | 1.25亿+论文库，自动化系统综述 | 系统性文献调研 | elicit.com |
| **Consensus AI** | AI驱动搜索，快速找证据 | 快速验证假设 | consensus.app |
| **Semantic Scholar** | AI提取论文含义，深度覆盖 | 探索研究脉络 | semanticscholar.org |
| **Scite** | 智能引用追踪(支持/反驳/提及) | 评估论文影响力 | scite.ai |
| **Connected Papers** | 可视化论文关系图 | 发现相关论文 | connectedpapers.com |
| **ResearchRabbit** | 论文推荐类似 Spotify | 持续追踪新工作 | researchrabbit.ai |

### 5.1 工作流建议

1. **初步探索**: Semantic Scholar + Connected Papers
2. **深度调研**: Elicit (自动化筛选和数据提取)
3. **证据验证**: Consensus AI + Scite
4. **持续追踪**: ResearchRabbit (新论文推送)

## 6. 多模型协作调研

使用多个 LLM 并行可以交叉验证，避免单一模型的偏见。

### 6.1 推荐配置

| 模型 | 角色 | 优势 |
|------|------|------|
| **Claude Opus 4.6** | 主力分析师 | 深度推理，长上下文 (200K) |
| **GPT-5.2 Thinking** | 漏洞检测者 | 捕捉被忽略的假设 |
| **Gemini 2.5 Pro** | 资料补充员 | 强大的检索和整合能力 |

### 6.2 协作模式

1. **分工阅读**: 不同模型负责不同子领域
2. **交叉验证**: 同一篇论文由多个模型独立分析
3. **冲突解决**: 冲突点往往是研究空白
4. **综合汇总**: 由最强模型整合所有发现

---

## 7. 个人高通量选题法（补充）

> 这部分来自 `AI research.md` 的个人实践段落，已融合为可执行版本，用于把"读很多论文"变成"筛出能发的 claim"。

### 7.1 三层过滤流程

1. **候选池层（广覆盖）**
   - 先建立 2000+ 论文级别的候选池，按任务、数据模态、评测协议分桶。
2. **可验证层（可落地）**
   - 对每个候选方向补齐：主实验、消融、压力测试、失败预案。
3. **可投稿层（可叙事）**
   - 只保留能形成完整论证链的方向：问题价值 → 方法机制 → 实验证据 → 反质疑预案。

### 7.2 快速打分模板（避免主观拍脑袋）

| 维度 | 问题 | 评分标准（1-5） |
|------|------|----------------|
| **Novelty** | 是否只是已有方法换壳？ | 5 = 明确新设定或新机制 |
| **Executability** | 两周内能否跑通主实验？ | 5 = 依赖清晰、实现成本可控 |
| **Evaluability** | 是否有公认基准和强 baseline？ | 5 = 可直接复现实验协议 |
| **Narrative Fit** | 能否写成完整论文故事？ | 5 = 贡献/证据/风险闭环完整 |

建议只推进总分最高的 Top 1-3 个方向，降低“多线开坑”风险。
