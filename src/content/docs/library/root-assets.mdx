---
title: 科研圣经
description: 零基础入门科研全指南 - 从顶会对比到Idea生成，从论文阅读到速成写作
sidebar:
  order: 1
---

# 科研圣经

本章节汇集零基础入门科研的核心知识，涵盖顶会顶刊对比、Idea生成方法、论文阅读流程与速成写作技巧。

---

## 一、顶会顶刊投稿对比

### 1. 审稿人意见分类

- Reject（拒稿）
- Major Revision（大修）
- Minor Revision（小修）
- Accept（接收）

**会议**：一般只有 Reject（拒稿）和 Accept（接收）两种

**期刊**：四种都有

> 意见及应对方法见：[知乎文章](https://blog.csdn.net/qq_39856931/article/details/106751954)

### 2. 审稿人提意见的意图

| 类型 | 意图 |
|------|------|
| **会议** | 提意见为了拒绝文章 |
| **期刊** | 审稿人提意见是为了改正文章，意见精确（跟哪些baseline进行对比、补充哪些评价指标、新增哪些数据集实验） |

### 3. 审稿人细致及专业程度

| 类型 | 特点 |
|------|------|
| **会议** | 很少有细节性的意见，质量参差不齐，不懂这个领域也可能是你的审稿人 |
| **期刊** | 认真细致，专业 |

### 4. 期刊和会议的优缺点

#### 会议

**时间较短**（顶会可能2-3月左右）

##### CVPR-2026 时间线

| 重要时间节点 | 阶段 |
|-------------|------|
| 2025年10月 | OpenReview投稿网站开放**作者注册** [OpenReview](https://openreview.net/) |
| 2025年10月 | OpenReview投稿网站开放**论文提交** |
| 2025年11月 | **摘要**提交截止（晚上11:59 UTC-12） |
| 2025年11月 | **完整论文**提交截止（晚上11:59 UTC-12） |
| 2025年12月 | **补充材料**提交截止（晚上11:59 UTC-12） |
| 2026年1月 | **第一阶段拒稿**通知 |
| 2026年2月 | 作者**反馈**窗口期 |
| 2026年3月 | **最终录用或拒稿**通知（主要技术轨道） |
| 2026年3月 | **最终版**（Camera-ready）文件提交（主要技术轨道） |
| 2026年6月 | CVPR-2026正式举办 |

##### AAAI-2026 时间线

| 重要时间节点 | 阶段 |
|-------------|------|
| 2025年6月16日 | OpenReview投稿网站开放**作者注册** |
| 2025年6月25日 | OpenReview投稿网站开放**论文提交** |
| 2025年7月25日 | **摘要**提交截止 |
| 2025年8月1日 | **完整论文**提交截止 |
| 2025年8月4日 | **代码/附录材料**截止 |
| 2026年9月8日 | **第一轮结果出炉**（首轮拒稿无rebuttal） |
| 2025年10月2-8日 | rebuttal阶段 |
| 2025年11月3日 | **最终录用通知** |
| 2025年11月13日 | **最终版**（Camera-ready）文件提交 |
| 2026年1月20-27日 | AAAI-2026正式举办 |

#### 期刊

**周期较长**（顶刊可能8月到1年左右不等）

LetPub 中科院一区 - 人工智能领域期刊查询结果

### 5. 顶刊顶会目录

**（1）2025年中科院新分区 1区 共53本期刊**

**（2）CCF会议分类**

[中国计算机学会推荐国际学术会议和期刊目录（2022更名版，2025年使用）.pdf](https://kyc.lsu.edu.cn/_upload/article/files/df/ed/9257ff9c401dabebfe7428c8a9ce/cc033863-6b2b-4616-8bab-ae183ed7a04b.pdf)

### 6. 计算机可投期刊信息查询及会议截止时间

#### （1）期刊

**LetPub** 是一个提供最新SCI期刊影响因子查询及投稿分析系统的网站，可按照期刊名、研究方向、影响因子、收录情况等条件筛选和排序

[网站地址](https://www.letpub.com.cn/index.php?page=./journalapp)

#### （2）会议

**CCF会议分类及截稿倒计时网站**

- 网站1：[ccfddl.cn](https://ccfddl.cn/) - 右上角可扫码关注小程序
- 网站2：[ccfddl.com](https://ccfddl.com/)

---

## 二、零基础入门科研如何想Idea

### 1. 一篇论文诞生的整体流程图

```
寻找领域内当前效果最好的论文（最好开源）
         ↓
看论文里展示的SOTA结果比较了哪些baseline
         ↓
顺着 baseline 与引用一路往前读
         ↓
在理论层面进行分析，找改进点
（哪些工作上不work，能否扩展）
         ↓
复现代码
         ↓
在代码层面进行分析，找缺陷
（运行速度慢、内存占用大等）
         ↓
改出自己的Idea
```

**举例**：寻找领域内当前效果最好的那篇论文，最好**开源**→看论文里面展示的**SOTA结果比较了哪些baseline**，顺着 baseline 与引用一路往前读→在**理论层面**进行分析，找改进点，例如在哪些工作上是不work的，能否在这个领域进行一些扩展→复现代码→在**代码层面**进行分析，找缺陷，例如虽然某方法性能有提升，但是运行速度很慢、内存占用大，能否进行加速和优化→改出自己的点子。

### 2. 寻找论文的渠道

#### （1）看最新出顶会中新中的论文

**官网**
- NAACL2025接收文章列表：https://2025.naacl.org/program/accepted_papers/
- CVPR2025接收文章列表：https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers

**GitHub科研总结**
- CVPR-2025-带代码的阅读论文：https://github.com/Paper2Chinese/CVPR-2025-reading-papers-with-code

#### （2）大机构数据库

- **arxiv平台**新出论文/预印本idea：https://arxiv.org/
- **谷歌学术**：https://scholar.google.com.hk/
- **dblp**：https://dblp.org/
- **Aminer**：https://www.aminer.cn/
- **huggingface**新出论文：https://huggingface.co/blog/zh?tag=research

#### （3）Paper with Code上面SOTA结果

网址：https://paperswithcode.com/

### 3. 建议部分

#### （1）阶段划分

1. **资格达成期**
   - **目标**：满足学位授予的硬性成果指标（篇数、等级）
   - **策略**：锁定 1–2 个垂直子领域，开展深度研究，尽量避免进入高曝光热门赛道，降低idea被抢占风险

2. **影响拓展期**
   - **目标**：在完成资格后，利用已积累的模型、工具链与领域认知，向相邻学科或跨模态任务延伸，追求更高学术影响力

#### （2）日常科研习惯

1. **输入端**
   - **文献精读**：每周 2 篇，输出结构化摘要（问题-方法-缺陷-可改进点）
   - **审稿意见逆向阅读**：收集公开评审意见，**提前预判可能被审稿人质疑点**

   **例如**：在 VLPose 这篇论文中，作者引入新模块的同时不可避免地引入了额外参数。为回应可能关于"性能提升是否仅源于参数增长"的质疑，作者在发表前通过对比实验设计，**在参数量可控的前提下**展示了其方法在性能上的显著优势，体现出结构设计本身的有效性。

   [论文链接](https://arxiv.org/pdf/2402.14456)

   **Table3(b)表**反映的是**在不同模型尺寸下，各微调方法需要训练的参数量**，单位为 MB。

   ##### 1. Visual Prompt Tuning（视觉提示微调）

   - **核心思想**：不改动模型本体，仅在图像输入的"前面"加上可学习的"提示 embedding"
   - 就像在 Transformer 里加了一个"引导标签"，帮助模型适应新任务
   - **只训练这部分新加的 prompt**，其余所有模型参数都 **被冻结（frozen）**
   - 这叫做**参数高效的微调方法**，很轻量，不会破坏原模型

   ##### 2. Last Layer Tuning（最后一层微调）

   - **传统做法**：固定前面的模型结构，只微调最后一层（通常是预测层）
   - 相比全参数微调，它已经是"轻量"了，但仍然要训练模型的一部分原始参数

   ##### 两种训练方式的不同点

   | 项目 | Visual Prompt Tuning | Last Layer Tuning |
   |------|---------------------|-------------------|
   | 是否改动原模型 | 否 不改，只加东西 | 是 改动最后一层 |
   | 可训练参数来源 | 全是新加的 prompts | 原模型中的部分参数 |
   | 参数量 | 通常更少 | 较多 |
   | 泛化能力保留 | 更强（原模型没动） | 中等（最后一层被改） |
   | 结构可逆性 | 可随时删除 prompts 恢复原模型 | 改过原模型不可逆 |

   **结论**是 VLPose 虽然只用了一点点额外的 prompt 参数（比 Visual Prompt 多，但远少于 Last Layer），但性能最好，结构还可逆，泛化也好。以高效的结构设计**消解了对"参数堆砌"的潜在质疑**。

2. **输出端**
   - **即时记录**：手机备忘录等
   - **定期研讨**：组会、会议、在线论坛等

### 4. 科研实操

#### 4.1 入门大模型怎么选方向

**目标**：先搞清楚"能做多大的实验"

按**资源**情况划分：

| 手里的卡 | 推荐任务 | 为什么要这样做 | 真实例子 |
|---------|---------|---------------|---------|
| **资源多** ≥100 张 A100 | 基于开源模型做大规模预训练、指令微调、强化学习 | 卡多=数据吞吐大，能把基线刷高 | LLaMA-65B 原版就是 2048 张 A100 训的 |
| **资源中等** 8 张 V100 | 小领域微调 | 卡不多不少，可以训"窄场景"专用模型 | **纯语言领域**：MoTCoder论文没有在humaneval或者mbpp这样的数据集上做验证，而是使用的Code Contest/apps：在专门做竞赛难度的编程数据集实验，数据量小但难度高。**多模态领域**：Lyra论文训练了一个视觉、语音、语言的多模态大模型，之前的多模态大模型在长音频和长视频上的效果不好，Lyra对应长视频音频的大海捞针任务做了提升 |
| **资源少** 4 张 3090 | 免训练（training-free） | 训练一次要一周，迭代慢；免训可直接推理，每轮迭代速度都很快，无需大量资源 | **纯语言领域**：Quick LLaMa：只做"长文本推理加速"，不改权重。**视觉领域**：VisionZIP：对图像的token做压缩 |

**MoTCoder论文链接**：https://arxiv.org/pdf/2312.15960

**Lyra论文链接**：https://arxiv.org/pdf/2412.09501

**QuickLLaMA论文链接**：https://arxiv.org/abs/2406.07528

**VisionZIP论文链接**：https://arxiv.org/pdf/2412.04467

#### 4.2 如何判断一篇论文是否值得精读

**值得精读 = 效果好 + 可复现**

##### （1）效果好

**情况1**：在某个Benchmark上面是SOTA

> **Benchmark**（基准测试）是指一套标准化的测试任务、数据集和评估指标，用于公平、可重复地比较不同方法或系统的性能。

**情况2**：不是SOTA，但是现有SOTA是在它的基础上进行改进

可能出现这样的情况，假设某论文A提出一个基础模型，后续论文B在A的基础上加了一个"花哨的模块"成为SOTA，但这个模块在数据量小或任务简单时反而拖后腿。所以可能基于论文A的baseline去改进效果会更好。

**情况3**：某工作是现有SOTA的基础或它启发了现有SOTA

如果不知道某论文是否和现有SOTA有关，可从以下4个方面来判断该篇论文的质量：

**方面1**：大机构 > 小机构

###### 常见的大机构类型

1. **顶尖高校及实验室**
   - 例如：MIT、Stanford、Tsinghua University等
   - 这些学校下的研究实验室（如MIT CSAIL、Stanford AI Lab）在学术圈具有很强影响力

2. **知名科研院所**
   - 例如：中国科学院、Max Planck Institute、Allen Institute for AI (AI2) 等

3. **科技巨头的研究部门**
   - 例如：Google Research（Gemini）、DeepMind、Microsoft Research（DeBERTa）、Meta AI（PyTorch、LLaMA）、OpenAI（GPT、CLIP）、NVIDIA Research（Diffusion Models）
   - 这些公司投入巨资，且产出大量SOTA研究成果

4. **开源社区与合作研究组织**
   - Hugging Face、OpenMMLab [香港中文大学多媒体实验室网址](https://openmmlab.com/codebase)

**方面2**：简单 > 复杂

复杂论文可能只是某个模块在起作用，作者为了使这篇论文工作更丰富创新，可能会融合很多模块，导致论文难以理解。

**方面3**：清晰 > 繁琐

优先论文有公式推导、伪代码和图示。

**方面4**：开源可复现

（2）可复现

优先有公开源码的论文，训练数据和训练参数齐全，近期公开/维护

#### Benchmark 是什么？

**基本定义**

- 在学术研究中，Benchmark 通常是：
  - 一个公开可用的 **标准数据集**（比如 COCO、ImageNet、HumanArt）
  - 配有 **明确的评价指标**（如 AP、Accuracy、F1-score）
  - 用于对不同模型/方法进行 **公平比较**

##### 举例

| 领域 | Benchmark 示例 | 作用 |
|------|---------------|------|
| 图像分类 | ImageNet | 比谁分类准 |
| 人体姿态估计 | MS COCO / HumanArt | 比谁关键点预测准 |
| 自然语言处理 | GLUE / SuperGLUE | 比谁语言理解好 |
| 大模型能力 | MMLU / GSM8K | 比谁多任务泛化能力强 |

一个完整的 Benchmark 往往包括：

| 组件 | 说明 |
|------|------|
| 数据集 | 统一的数据输入（比如图片、文本） |
| 标注 | 统一的"正确答案"或ground truth |
| 评价指标 | 用来打分，比如AP、Recall等 |
| 测试协议 | 比如是否允许使用额外数据、是否使用ground truth框等 |

#### 4.3 看论文的流程

##### （1）标题：大致知道是做什么方面的工作

译为：QuickLLaMa：用于大型语言模型的查询感知推理加速

通过QuickLLaMa：知道大致在做一件加速的事情

##### （2）摘要&示意图：大致知道这篇论文在做什么事情

用户询问了关于哈利波特中很细节的问题，普通的Llama3无法给出回答，但是QuickLlama可以在30s内阅读完一本100k token的哈利波特并给出准确回答。

如果对这篇文章不感兴趣的话，可以去看下一篇文章了。

##### （3）深入看方法/架构示意图：知道这篇论文大致是在用什么框架/方式来实现这件事情

###### 图示关键词解释

| 名称 | 含义 | 用途 | 举例 |
|------|------|------|------|
| Global Tokens (G) | 模型运行时始终存在的全局信息，如系统提示、任务说明等 | 作为整个推理过程中每一步都可访问的公共信息；增强模型对任务和角色的理解 | 系统提示："你是一个图书问答助手" |
| Query Tokens (Q) | 用户输入的问题或查询句 | 用来指导模型重点关注哪些记忆块（Memory Blocks） | "哈利第一年结束时金妮指向了谁？" |
| Context Tokens (C) | 模型历史接收过的长文本内容，被拆分成很多 memory blocks 存入"记忆"中 | 构成"记忆仓库"；供后续从中查找与 Query 或当前生成位置相关的上下文信息使用 | 整本《哈利·波特》的文本内容被切分成块 |
| Memory Blocks (B) | 上述 Context Tokens 划分后的一块块"笔记段落" | 用来支持查询感知（query-aware）的查找：哪些内容是现在需要调出来使用的 | Block 1 是"火车站送别"，Block 2 是"魔法课" 等 |
| Query-related Blocks (R) | 与 Query 或 Current Token 高度相关的 memory blocks | 是从 Memory Blocks 中"筛选"出的重要内容，用于当前推理窗口 | 当前提问与"火车站"段落相关，系统只加载这几段 |
| Local Tokens (L) | 当前生成位置前的短距离上下文 token（相对于 Current Token） | 让模型知道"现在在哪"，并为相关性打分提供"当前位置的上下文" | "昨天张三来了，他…" 是生成"是"之前的 L |
| Current Token (H) | 当前要生成/预测的 token 的隐藏向量表示（注意力机制中的 query 向量） | 用于决定当前哪些 memory block 有用（用于 s(B, H) 打分） | 当前正要生成当前准备生成/预测的那个 token"他是___"的"是"这个词 |
| Current Key-Value Cache (M) | 当前这一步模型使用的 key/value 信息组合，包含 G, Q, R, L 四部分 | 作为 Attention 层的输入；也是每一步模型做决策时真正"看到"的全部上下文 | 当前模型输入 = 全局说明 + 问题 + 相关段落 + 上下文 |

**疑惑**：先有的RAG和QuickLLaMa谁出现时间较早？它们有什么区别？

——>使用AI工具，用kimi k2查询得到以下对比表格：

| 维度 | RAG(2020年) | QuickLLaMA（2024年） |
|------|-------------|---------------------|
| 核心机制 | 在生成前，**先检索外部文档**，再让 LLM 结合检索结果生成答案 | **在生成过程中**，动态从长上下文中**查找与查询相关的片段** |
| 数据来源 | 通常是**外部知识库**（如 Wikipedia、向量数据库） | 是**当前输入的长文本本身**（如一本书、一篇长文） |
| 是否需要训练 | 需要训练检索器和生成器 | **无需训练**，可直接插入现有 LLM |
| 使用场景 | 回答**知识密集型问题**（如"2024 年奥运会举办城市"） | 回答**长文本内部问题**（如"哈利·波特中谁最后拿到魔杖？"） |
| 检索粒度 | 以**文档或段落**为单位 | 以**token 或记忆块**为单位，更细粒度 |

##### （4）看实验表格结果：看这篇论文的方法比较的baseline有哪些，明确这论文的**优势和提升**是什么

图一第一列展示了，不同 benchmark 中的任务（多项选择问答、阅读理解题、对话摘要...）

第一行：方法对比；第二行：上下文窗口大小（Context Window）

该方法可以在长文本输入的Benchmark（基准测试）上取得很好的效果。

| 方法名称 | 是否需要重新训练 | 是否 Query-aware（查问题重点） | 推理速度 | 显存占用 | 记忆能力 | 综合表现 |
|---------|----------------|------------------------------|---------|---------|---------|---------|
| LLaMA3-8B-1048K | 需要 | 否 | 慢 | 极高 | 很强（靠训练） | 精度高但代价大 |
| StreamingLLM (Stream) | 不需要 | 否 | 快 | 中等 | 差（会忘） | 快但记不住长文本 |
| InfLLM | 不需要 | 否 | 快 | 低 | 一定记忆力 | 比 Stream 更强，但不够聪明 |
| QLLM (QuickLLaMA) | 不需要 | 是 | 最快 | 最低 | 最强（精准查找） | 表现最全面优秀 |

图二展示了该方法在显存占用和时间上面和baseline的对比，Llama3都显存占用会随着token数量增加呈现指数增长，很快就会OOM超显存，但本文方法最多只使用了20b左右的显存，且时间增长是线性的。故有此结论，该方法相比于之前的baseline在性能、显存占用、速度都有提升。

如果对这篇文章不感兴趣的话，认为某个步骤写的不好，可以去看下一篇文章了

##### （5）进一步看表格内部的baseline：找baseline对应论文，用上面的流程再读一遍

比如这里比较了英伟达的Stream LLM，去找这篇baseline对应的论文，用上面的流程再读一遍，把表格里面所有的baseline都搞懂，就差不多理解当前这篇论文了

##### （6）正文/代码：想有哪些地方可以改进，基于当前文章产生一个idea

#### 4.4 代码跑通流程

（1）去 Papers with Code /github等搜索任务关键词。

（2）选有官方代码+有训练脚本+最近一年有更新的论文代码（有readme文件、环境版本号具体、参数详细、训练验证等使用教程清晰）。

（3）复现 = 配环境 → 下载数据 → 跑脚本 → 得到和作者差不多的分数。

**详细流程**：配置好代码环境和数据 → 理解代码运行逻辑（从主函数开始，哪些部分是数据处理，哪些是核心算法） → 基于哪些部分进行改进 → 对核心部分插入断点

##### 阶段1 代码理解

例如：MoTCoder这篇论文中的模型，通过拆分子模块的方式来对代码问题进行求解，那么你可以输入给他一个例子，观察它是如何对输入问题进行子模块的拆分，从而理解这个模型的运行逻辑。

如果代码库提供可视化，可以基于可视化来帮助理解。

##### 阶段2 在代码理解之上改进

基于现有代码进行改进，比如观察到论文中的attention热力图对某些局部信息没有体现，你就可以新增一个模块来重点关注这些局部细节信息。经过你的新增模块之后，再次绘制热力图，发现之前没有被标注的细节信息被highlight，那么可以证明你新增的模块是有效的。

**例子**：论文TagCLIP新增了一个Trusty Learner模块，作者在论文中配了对应的可视化示意图，来展示该模块的作用，用以验证该模块的有效性。

**论文研究问题**：现有的基于对比语言-图像预训练（CLIP）的零样本语义分割方法在处理未见类别时存在显著的误分类问题，尤其是容易将未见类别与语义相似的已知类别混淆。

**对于阶段2例子的详细解释**

1. **研究问题：模型会把"没见过的东西"当成"见过的"**

研究的核心问题是：在图像分割中，模型需要给每个像素打上正确的类别标签。但**很多模型只见过训练集里的"已知类别"**（比如飞机、汽车、牛），一旦遇到"没见过的类别"（比如盆栽、电视、沙发），它**常常会误认为这些新东西也是已知类别**。比如，它可能会把"盆栽"误认为是"瓶子"，因为两者颜色或形状有点像。

2. **解决方法：让模型学会"这是不是我认识的东西"**

TagCLIP 的方法是增加了一个 **"Trusty Token"**，它的工作就像一个"可信度检测器"——**先判断这个像素是不是已知类别**。如果它觉得"不像是我认识的东西"，就会**抑制**模型给它贴上已知类别的标签。

3. **为什么要放 Fig.4？**

**Fig.4 就是用来证明这个"可信度检测器"真的好用。**

- 图里左两列显示，检测器对"已知类别"的反应很明显（像飞机、汽车这些它认识的东西被高亮）
- 右一列显示，它对"未知类别"几乎不亮，说明它能**有效忽略没见过的东西**

简单说，**这张图是"证据"**，让读者一眼就能看出：

> "看！我们的新方法确实能分清什么是它见过的，什么是它没见过的。"

#### 4.5 怎么读代码

**目标**：搞清楚"哪一段代码是我未来要改的地方"

**流程**：

##### 情况1：普通github代码

整体框架粗读，重点部分代码亲自跑一遍（插断点），一行行去看其中的变量是如何变化的、变量的形状是什么。

##### 情况2：代码库

比如在**大模型训练**中常见的**代码库LLaMA-Factory**

或者在视觉检测中常见的**MMDetection**，但这个代码库的封装很厉害，找变量入口和变化方式都很麻烦。因此这种类型的库不必层层debug，无需完全理解它的代码运作流程，**会用即可**。在使用说明里面看，如何添加一个新的模型、数据集等。

**（1）粗读**：

打开github仓库，先看 `README.md`，找到入口脚本（通常叫 `main.py` 或 `train.py`）。

**（2）细读**：

在核心函数里打断点。

1. **IDE**：用 VS Code 打开代码，`F9` 打断点，`F5` 启动调试
2. **断点**：让程序跑到这行就暂停，方便查看变量值
3. **变量形状**：`print(x.shape)`，例如 `[32, 128, 768]` 表示 batch=32、序列长 128、特征维度 768

**具体例子**：在QuickLLaMA这篇文章中，它的核心框架是attention部分的实现，在代码中嵌套了方程来代替原本模型attention中的forward函数。除了论文中实现的QLLM之外，代码库还包含了这个方向的一系列论文，比如StreamLLM、LM-Infinite、InfLLM这些代码都是用同样的框架实现的，因此可以很容易比较他们之间的不同。

[StreamLLM论文](https://arxiv.org/pdf/2309.17453)

[LM-Infinite论文](https://arxiv.org/pdf/2308.16137)

#### 4.6 改进现有SOTA

##### 从论文方法角度进行分析

效果好的论文，分成三类：

**（1）效果好，方法创新——顶级论文**

能否把这个方法用在新领域，比如Transformer出了之后——>做vision Transformer、Mamba出了之后——>做vision Mamba、Sam出了之后——>做3D Sam。

- **优点**：如果是第一个把这个方法用在新领域的人，会受到很多关注
- **缺点**：竞争大，各大机构都在同台竞技

**（2）效果好，方法简单**

- **缺点**：方法简单但改进有难度

**法1**：可以把一项工作尝试用在新领域

MOOD的这篇CVPR论文就是把掩码图像建模用在了异常检测方面。掩码图像建模方法在很多领域都证明了其有效性，但未在异常检测领域进行应用，可以实验证明该方法在异常检测的各个方向上是有效的。

**法2**：把这篇论文和其他论文的方法相结合（不只是ABC模块组合，也可以是A模块+B方法+C评价指标）

比如MOODv2就是把**MOOD结合**上**新的预训练策略BEiTv2**再**结合新的异常检测指标ViM**

**（3）效果好，方法复杂**

因为方法复杂，尝试把方法进行拆解，把某些**模块替换**成你觉得更好的模块或把某些**模块简化**

##### 两种通用的方法

**法1**：找到现有方法在某些场景不work的缺陷，去掉现有方法成立的一些必要条件

现有方法缺陷：比如QuickLLaMa这篇，核心框架是：利用问题在长文本中进行关键信息的查找，有一种情况是，如果用户先输入的是长文本但是却不输入问题，在这种情况下可以改进这一点来形成新的文章。

**核心思路就是：去掉现有方法成立的一些必要条件。**

在新的场景下，这些现有方法就不work了，你就可以针对这种新场景来完成一篇论文。

**法2**：关注新出的Benchmark

比如HumanArt数据集是第一个在艺术领域上的姿态检测数据集，之前的人体姿态估计方法在这个数据集上的效果不好，于是出现了VLPose这个方法，在艺术检测这个数据集上和通用数据集上都做出了改进，形成一篇新论文。

**法3**：添加新的Benchmark、消融实验、Task

当idea已经证明可行，但性能不是SOTA时：

- **前提建议**：选题阶段即对标SOTA

- **已验证有效但非SOTA性能：改进与发表策略**

**（1）尝试和现有SOTA相结合**

**（2）无法结合SOTA时考虑：是否必须对比？**

可能**无需比较**的情况：

- 情况1：是arxiv的预印本，还没有中稿
- 情况2：是同期工作

**必须比较**的情况（想尽办法去证明你的工作是有价值的）：

**方法1**：添加新的 Benchmark（覆盖更多任务维度，提升普适性说服力）

- 某单一任务性能不如SOTA，但在其他10个任务上表现领先
- 在多跳推理上优于SOTA，虽然单跳问题上稍逊

**方法2**：加新的消融实验（精细化对比同一设置下的表现）

**原则**：

- 比较对象限定为同等规模模型/方法
- 若你贡献在于训练策略，仅需对比其他训练策略
- 开源只和开源比，小贡献但有价值

**方法3**：添加新的task（做别人做不到或未做之事，强调独特性）

- 多模态模型实现视觉+语音+语言，领先仅支持视觉+语言的方法
- 能处理2小时视频而非其他方法仅支持2秒视频
- 引入新语言/场景/规模等独特任务设定

**总结**

若以上方法均尝试后性能仍无竞争力，该idea或许尚不成熟。

**科研初期优先级建议**：

- **效果优于创新**：方法普通但效果好，也值得发表
- **工程贡献也是贡献**：通过数据工程、训练技巧取得SOTA，也会获得关注

> 例如：无需新颖方法，仅凭高质量数据 + 大参数量 + 微调策略也能发表高水平成果（如许多大模型训练类论文）。

#### 4.8 怎么把论文的故事讲好

**目标**：让审稿人快速相信"你的工作合理且有效"

##### 解释为什么A+B是有效的

**（1）理论上**

验证A+B > A，用公式推导进行证明（最有说服力，但很多情况下无法用理论求解）

**（2）现象上**

观察到了A中的一些现象，B这一模块可以弥补这一缺陷，为什么能够弥补，可以搭配一些可视化，与**4.4 代码跑通流程阶段2 在代码理解之上改进**这部分的原理是相同的

**（3）工程上**

把系统中的每个模块都相对应的**比较同期模块**，验证你的系统所采用的模块是最有效的。

**例如** MOOD这篇论文比较了不同的预训练策略、不同的模型架构、不同的OOD检测指标，证明在这个系统中所使用的每一个模块都是最优的。

**什么是OOD检测？**

在现实世界中，很多AI系统面临这样的问题：它们在训练时只见过一部分"已知的数据"（称为**In-Distribution（ID）数据**），但测试或应用时可能会遇到"陌生的数据"（即**Out-of-Distribution（OOD）数据**）。OOD检测的任务就是要让AI系统学会识别出这些陌生的、没见过的数据，并避免错误地对它们做出判断。

**（4）总结**

把论文的**形成过程**讲清楚，包括整个**系统的脉络**、以及**每个模块的作用**、每个模块都要配合对应的**消融实验**来验证模块的有效性

---

## 三、怎么3天速成一篇论文

### 工具

- **GPT-4o**（英文） + **DeepSeek**（中文）
- **LaTeX格式论文在线编译网站**：[Overleaf](https://www.overleaf.com/login)

[Overleaf知乎简介](https://zhuanlan.zhihu.com/p/694594439)

### 应用举例

**（1）给图片写标题**：向GPT输入：图片＋提示词（例如：给图片写英文latex格式的caption）

**（2）写相关工作**：相关工作的每篇论文摘要粘贴下来，输入给GPT可生成一个相关工作初稿

**（3）介绍数据集**：将数据集相关介绍输入给GPT生成data部分的段落

**（4）组会汇报**：做PPT可以使用DeepSeek：把一周工作内容输入给DeepSeek做PPT，将生成的表格直接粘贴到PPT中，提高工作效率
