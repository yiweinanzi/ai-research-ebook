---
title: AI research 全量原文
description: 根目录 AI research.md 的完整内容（未删减，原文归档）
sidebar:
  order: 1
---

> 本页完整收录根目录 `AI research.md` 原文，不做删减。为了保证渲染稳定，原文以代码块方式展示。

````markdown
# Vibe Research

## 1. idea

### 1.1 调研

这一部分的目标不是“看了很多论文”，而是要产出**能直接导向可写论文的结果**：
1）该方向到底在解决什么问题；

2）大家默认的设定/评测是什么；

3）现在的瓶颈在哪里；

4）哪些空白值得写；

5）你的 idea 能否形成可验证的 claim。

#### 1.1.1 工具选择与分工（多模型交叉验证）

- **主力调研工具：ChatGPT 5.2 Pro**
  - 适合做：体系化综述、把一堆材料整理成“研究地图”、提炼问题定义与评价体系、把想法变成可验证 claim。
- **辅助：Manus / GPT 5.2 Thinking / Gemini 3 Pro Deep Research**
  - 适合做：补全资料、扩展相关工作、从不同角度找“被忽略的假设”“隐藏的 trade-off”“潜在新设定”。

> **推荐分工（避免重复劳动）**
>
> - Pro：搭主框架（问题→方法→实验→结论）+ 产出“标准化笔记”
> - Thinking：抓漏洞（这篇论文到底新在哪里？有没有偷换设定？评价是否公平？）
> - Deep Research：补引用与证据（相关工作漏了谁？还有哪些强 baseline？）

#### 1.1.2 调研的“产物清单”（做完一定要得到这些）

建议把调研结果固定成 6 份输出，这样后面推进会很顺：

1. **Related Work Map（研究地图）**
   - 按“问题子类/数据模态/方法范式/训练范式/评测范式”分层。
2. **Benchmark & Metric Sheet（评测与指标表）**
   - 主流数据集/指标/协议/常见坑（比如数据泄漏、prompt 泄漏、训练-测试重叠等）。
3. **Assumption Matrix（隐含假设矩阵）**
   - 每篇论文默认假设是什么？（资源、标注、算力、可访问信息、交互形式、闭环方式）
4. **Failure Taxonomy（失败模式分类）**
   - 方法为什么失败？失败发生在什么条件下？是否可通过设置改变暴露出来？
5. **Gap List（研究空白清单）**
   - 空白不是“没人做”，而是“没人把它做成可验证主张 + 可复现实验”。
6. **Claim Candidate Pool（候选主张池）**
   - 每个 claim 必须能写成一句话，并且对应至少一个主实验 + 一个消融/压力测试。

#### 1.1.3 调研流程（从“看论文”到“产出空白”）

你可以用下面 5 步跑完整调研闭环：

1. **定义边界（Scope）**

- 你要解决的“核心问题”一句话是什么？
- 哪些工作算“同类问题”，哪些只是“相关但不同问题”？（避免综述发散）

1. **建语料库（Corpus）**

- 2020–2026 的论文抓下来只是起点；关键是要做**可检索、可统计、可筛选**的结构化库。
- 每篇至少记录：任务定义、输入输出、数据集、指标、训练资源、关键 novelty、关键 baseline。

1. **标准化阅读（统一模板）**

- 不要自由笔记，要“字段化”：
  - Problem：解决什么，设定是什么？
  - Key Idea：一句话新意是什么？
  - Mechanism：为什么有效（作者的因果叙事）？
  - Evidence：哪些实验真正在支撑 claim？
  - Baselines：对比是否公平？
  - Weakness：最大漏洞是什么？
  - Reproduce：复现难点在哪里？

1. **对齐与冲突检测（Cross-check）**

- 多模型调研结果可能冲突，冲突往往就是“机会点”。
- 把冲突点整理成：
  - “该方向究竟默认设定是什么？”
  - “某些工作是不是在换设定？”
  - “指标是不是被 hack 了？”
  - “某些强 baseline 是否被刻意忽略？”

1. **产出 gap → 变成可写 idea**

- 每个 gap 最终要落到**可写论文的形式**：
  - Claim（主张）
  - Setup（设定）
  - Method（方法）
  - Experiment（实验）
  - Expected Outcome（预期）
  - Risk（最大风险与备选方案）

#### 1.1.4 文献检索工具 (2025最新)

| 工具 | 核心功能 | 适用场景 | 链接 |
|------|---------|---------|------|
| **Elicit** | 1.25亿+论文库，自动化系统综述 | 系统性文献调研 | elicit.com |
| **Consensus AI** | AI驱动搜索，快速找证据 | 快速验证假设 | consensus.app |
| **Semantic Scholar** | AI提取论文含义，深度覆盖 | 探索研究脉络 | semanticscholar.org |
| **Scite** | 智能引用追踪(支持/反驳/提及) | 评估论文影响力 | scite.ai |
| **PaperPal** | 2.5亿+引用，学术写作助手 | 写作时的引用检查 | paperpal.com |
| **Connected Papers** | 可视化论文关系图 | 发现相关论文 | connectedpapers.com |
| **ResearchRabbit** | 论文推荐类似 Spotify | 持续追踪新工作 | researchrabbit.ai |

**工作流建议：**
1. **初步探索**: Semantic Scholar + Connected Papers (快速建立地图)
2. **深度调研**: Elicit (自动化筛选和数据提取)
3. **证据验证**: Consensus AI + Scite (验证引用质量)
4. **持续追踪**: ResearchRabbit (新论文推送)

#### 1.1.5 多模型协作调研工作流

使用多个 LLM 并行可以交叉验证，避免单一模型的偏见：

**推荐配置：**

| 模型 | 角色 | 优势 |
|------|------|------|
| **Claude Opus 4.6** | 主力分析师 | 深度推理，长上下文 (200K) |
| **GPT-5.2 Thinking** | 漏洞检测者 | 捕捉被忽略的假设 |
| **Gemini 2.5 Pro** | 资料补充员 | 强大的检索和整合能力 |

**协作模式：**
1. **分工阅读**: 不同模型负责不同子领域
2. **交叉验证**: 同一篇论文由多个模型独立分析
3. **冲突解决**: 冲突点往往是研究空白
4. **综合汇总**: 由最强模型整合所有发现

------

### 1.2 调研项目

#### 1.2.1 Sci-Reasoning

《Sci-Reasoning: A Dataset Decoding AI Innovation Patterns》

> 这篇论文提出了 **Sci-Reasoning 数据集**，通过 LLM+人工校验的方法，对 2023–2025 年 NeurIPS、ICML、ICLR 的 3819 篇 Oral/Spotlight 论文进行“学术谱系”建模，系统标注论文如何从前人工作中发现缺口、跨领域融合和重构表示，并总结出 15 种科研思维模式，其中“问题重构”“跨领域融合”和“表示转化”占主导，揭示了 AI 研究创新的核心规律，并为训练自动化科研智能体提供结构化推理数据支撑。

代码：<https://github.com/AmberLJC/Sci-Reasoning>
数据集：<https://huggingface.co/datasets/AmberLJC/Sci-Reasoning>

![1769678224646](../AppData/Roaming/Typora/typora-user-images/1769678224646.png)

**你能从它身上"直接借用"的点：**

- 把"创新"从玄学变成**可标注的轨迹**：
  - 缺口识��� → 表示/设定重构 → 机制引入 → 实验验证
- 给你一个非常实用的启发：
  - 你做自己的调研库时，也可以把每篇论文的"创新类型"做标签化（后面 pattern mining 很好用）。

**15种科研思维模式完整列表：**

| 编号 | 模式名称 | 比例 | 描述 |
|------|---------|------|------|
| P01 | Gap-Driven Reframing | 24.2% | 转化限制为设计约束，将失败模式显性化 |
| P02 | Cross-Domain Synthesis | 18.0% | 跨领域融合，引入相邻领域的解决方案 |
| P03 | Representation Shift | 10.5% | 表示转化，替换问题基本表示单元 |
| P04 | Modular Pipeline Composition | 4.2% | 模块化组合，分解复杂系统 |
| P05 | Data & Evaluation Engineering | 5.4% | 数据与评测工程，创建新基准 |
| P06 | Principled Probabilistic Modeling | 5.4% | 概率建模，形式化框架 |
| P07 | Formal-Experimental Tightening | 6.7% | 理论实验紧密结合，迭代验证 |
| P08 | Approximation Engineering | 4.9% | 可扩展性近似，替换不可计算操作 |
| P09 | Inference-Time Control | 2.4% | 推理时控制，无需重训练 |
| P10 | Structural Inductive Bias | 5.1% | 结构归纳偏置，硬编码领域知识 |
| P11 | Multiscale Modeling | 1.4% | 多尺度建模，多粒度层次 |
| P12 | Mechanistic Decomposition | 1.9% | 机制分解，可解释性分析 |
| P13 | Adversary Modeling | 1.5% | 对手建模，防御��重用 |
| P14 | Numerics & Systems Co-design | 1.4% | 数值与系统协同设计 |
| P15 | Data-Centric Optimization | 2.1% | 以数据为中心的优化 |

**创新组合模式 (Innovation Recipes)：**

研究发现了高频组合模式，这些"配方"更容易产生高质量工作：

| 组合 | 出现次数 | 策略 |
|------|---------|------|
| Gap-Driven + Representation Shift | 318 | "诊断+重构"：识别问题，换表示解决 |
| Cross-Domain + Representation Shift | 233 | "导入+适配"：跨域引入，修改表示 |
| Gap-Driven + Cross-Domain | 204 | "诊断+借用"：识别缺口，跨域寻找方案 |

**会议偏好分析：**

- **ICML**: 偏好形式化方法 (8.3% Formal-Experimental, 7.5% Probabilistic)
- **ICLR**: 强调表示创新 (11.8% Representation Shift, 8.5% Benchmarking)
- **NeurIPS**: 平衡跨学科覆盖

**时间趋势 (2023-2025)：**

- Representation Shift 在2024年达到峰值 (8.0% → 11.5%)
- Formal-Experimental Tightening 逐年下降 (10.1% → 7.1% → 6.6%)
- Data & Evaluation Engineering 呈现增长趋势

**LLM 评估基准 (Hit@10)：**

- Gemini 2.5 Pro: **49.35%**
- Claude Opus 4: **42.86%**
- GPT-5.2: **38.89%**
- Claude Sonnet 4: **29.87%**

> 这个评估衡量：给定前人工作，LLM 能否预测出研究方向的准确度。

#### 1.2.2 AI Research

《Towards Execution-Grounded Automated AI Research》

> 这篇论文提出了"执行落地"（Execution Grounding）的思路——不仅让AI提想法，还得让它自己把想法变成代码、跑实验、看结果、从反馈中学习。研究团队搭建了一个完整的自动化执行系统，能并行运行数百个GPU实验，真正把"想法-实现-验证-改进"这个闭环跑通了。

代码：<https://github.com/NoviScl/Automated-AI-Researcher>

**你能从它身上“直接借用”的点：**

- idea 不是终点，必须落到“可运行”的实验闭环：
  - 不然你很容易停留在“看起来很新”，但没法被验证/复现。
- 自动化的价值：
  - 快速做 ablation、stress test、超参扫描，把"可信度"堆出来。

#### 1.2.3 Idea2Story

《Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives》

> 这篇论文提出了一个自动化流水线，将研究概念转化为完整的科学叙事。从想法到完整论文，19位作者合作构建了端到端的自动化系统。

**核心能力：**
- 自动生成完整论文结构
- 从概念到叙事的智能转换
- 保持科学严谨性的同时提升写作效率

#### 1.2.4 The AI Scientist 系列

自动化科研领域的最新进展：

| 项目 | 发布时间 | 核心特点 |
|------|---------|---------|
| **The AI Scientist** | 2024.08 | 完整研究自动化：idea生成→实验→论文写作→同行评审 |
| **The AI Scientist-v2** | 2025 | 无代码模板的 workshop 级别自动化发现 |
| **ChemCrow** | 2024 (Nature MI) | 化学研究 Agent，18种集成工具，728+引用 |
| **Virtual-Scientists** | ACL 2025 | 端到端科研协作模拟，团队组织→idea生成 |
| **Kosmos** | 2025 | 自主数据驱动的科学发现 |
| **AI-Researcher** | 2025 | 完全自主的研究系统 |

**代码资源：**
- The AI Scientist: https://github.com/SakanaAI/AI-Scientist
- Virtual-Scientists: https://github.com/InternScience/Virtual-Scientists

#### 1.2.3 个人思路（从 2000+ 篇论文挖出最可能发的 idea）

你当前这段思路很好，我这里把它变成**更可执行的流水线**，并补上“结构化字段”“pattern mining 怎么做”“输出应该长什么样”。

##### A. 语料收集与结构化（2020–2026 全量抓取只是第一步）

- 目标：把论文变成一个可统计的数据库，而不是“PDF 堆”。
- 每篇论文的最小字段建议这样定（够用且不重）：

1. **Meta**：标题、年份、会议/期刊、代码链接、数据链接
2. **Problem**：任务定义、输入输出、约束条件
3. **Setting**：训练资源、标注类型、是否闭环、是否可交互、是否多模态/多智能体
4. **Method**：核心模块、关键设计、训练目标
5. **Claim**：作者声称的贡献（拆成 1–3 条）
6. **Evidence**：哪些实验在支撑哪条 claim（强制对齐）
7. **Baselines**：对比对象 + 是否公平（同设定/同预算/同数据）
8. **Weakness**：最大漏洞/最脆弱假设/最容易被喷的点
9. **Innovation Type（创新类型标签）**：
   - 问题重构 / 设定变更 / 表示转化 / 机制引入 / 跨域融合 / 训练策略重组 等

##### B. “创新轨迹”显性化（抽丝剥茧的可操作版本）

你原本的 5 条非常关键，我这里把它们变成**每篇论文必填的问题**：

1. **它建立在哪些前人工作上？**
   - 不是只列 citation，而是写“继承了哪个关键设定/指标/模块”。
2. **作者看到了什么别人没看到的？**
   - 通常是：某个失败模式、某个隐含假设、某个不被重视的指标、某个跨域映射。
3. **它怎么把旧东西组合成新东西？**
   - 组合方式尽量标签化：替换/拼接/重参数化/对齐/约束化/分解-重组。
4. **把隐藏在论文里的思考过程显性化**
   - 强制输出：
     - “如果没有 A 假设，方法还成立吗？”
     - “如果指标换成 B，会不会崩？”
     - “如果预算翻倍/减半，优势还在吗？”
5. **对创新轨迹做 pattern mining**
   - 目的不是写综述，而是产出“可复用的创新范式”，进一步生成候选 idea。

##### C. Pattern Mining 怎么落地（别停在口号）

你说“对三千多条创新轨迹做 pattern mining”——这里给一个非常直接的做法（不用过度复杂）：

- **Step 1：把创新标签 + 文本摘要向量化**
  - 标签（离散）+ 摘要（embedding）组合。
- **Step 2：聚类/主题发现**
  - 看看高频组合：比如“设定变更 + 训练目标重构”是否经常一起出现？
- **Step 3：找“高频但未被系统化”的套路**
  - 例如：大家都在做某种 trick，但没人把它抽象成一个可解释的机制 + 标准化评测。
- **Step 4：从簇里反推 gap**
  - 哪些簇缺强 baseline？
  - 哪些簇缺统一评测？
  - 哪些簇只在一个数据集有效？（缺泛化证据）

##### D. 最终输出（你要得到的不是“结论”，而是“候选课题列表”）

建议最后输出一个 **Idea Shortlist（10–20 个）**，每个 idea 固定这几个字段：

- **一句话 claim**：我要证明什么？
- **为什么现在能做**：数据/模型/工具/计算力是否成熟？
- **最小可行实验（MVP）**：1 周能跑出初步结果吗？
- **关键 baselines**：必须打败谁？
- **风险点**：最大失败原因是什么？
- **Plan B**：如果主假设不成立，还有什么可转向的改造？

------

## 2. 代码

你的“项目文档模板”非常强。这里我补两点，让它更像一个**能指导落地的执行系统**：

1）项目文档除了“写作基准”，还要承担“工程基准”；
2）必须把“实验闭环”写进模板里（否则容易只写出概念）。

将 idea 交给 gpt pro/gpt thinking 之后，让其生成可用的项目文档，按照下面的格式（示例）：

> **0. 一页速览（给合作者/导师/审稿人 30 秒读懂）**
>
> **1. 研究问题与动机（Problem Statement）**
>
> **2. 相关工作地图（Related Work Map）**
>
> **3. 核心研究假设与可验证主张（Claims → 必须对应实验）**
>
> **4. 形式化：从 Memory 管理到 CMDP（Problem Formulation）**
>
> **5. 系统设计：MACRO-Mem 架构（System Architecture）**
>
> **6. 学习目标：奖励、约束与训练稳定性（Learning Objective）**
>
> **7. Reward Model（RM）模块：训练、校准、门控与防 hacking**
>
> **8. Counterfactual VoM：定义、估计、用途（Training + Interpretability）**
>
> **9. 训练算法：Constrained RL + step-wise credit assignment（Algorithm）**
>
> **10. 实验与评测：主表、消融、压力测试、安全（Experiments）**
>
> **11. 评测与指标定义（Metrics Specs）**
>
> **12. 可能的审稿人质疑与预案（Reviewer Objections → Preemptions）**
>
> **13. 工程落地：代码结构、接口、复现协议（Implementation）**
>
> **14. 路线图：20 周拆解 + Go/No-Go Gate（更像项目管理而不是“愿望清单”）**
>
> **15. 论文大纲模板（可直接粘到 NeurIPS 写作中）**
>
> **16. 附录：参考文献清单（建议写论文时按类别组织）**
>
> **17. 变更说明：v2.2 相对 v2.1（供你对照检查）**
>
> **18. 基准对接手册（Benchmark Cookbook：一键跑通 + 常见坑）**
>
> **19. Baseline 复现与公平对比协议（Fair Comparison Protocol）**
>
> **20. 候选生成器与冲突检测：实现规格（Candidate/Conflict Spec）**
>
> **21. 隐私与安全：从“评测”到“训练约束”的闭环（Privacy & Safety Loop）**
>
> **22. 训练配方（Training Recipe：建议写进 Appendix，避免被喷不透明）**
>
> **23. 记录与可视化（Logging & Visualization：让分析更像 NeurIPS）**
>
> **24. 论文图表清单（Paper Assets Checklist）**
>
> **25. 你现在可以立刻做的 10 件事（无需等我/无需额外信息）**
>
> **26. 工具调用与 Prompt 模板（Tool Schema & Prompts）**
>
> **27. 数据与轨迹格式（Trajectory Format：用于 RM & RL）**
>
> **28. 失败模式分类（Failure Taxonomy：用于 analysis & safety）**
>
> **29. 开源与合规建议（Open-sourcing & Compliance）**

#### 2.0（补充）让“项目文档”更能落地的两个增强段

**A. Repo Skeleton（建议写进第 13 节 Implementation）**
把代码结构提前定死，后面 Claude/Codex 才能稳定执行：

- `datasets/` 数据下载与预处理（带 hash 校验）
- `models/` 核心模型与模块
- `configs/` 实验配置（确保可复现）
- `train.py / eval.py` 统一入口
- `scripts/` 一键复现实验脚本
- `results/` 自动汇总主表 + 图表
- `docs/` 论文图表、方法说明、复现指南

**B. Experiment Loop（建议写进第 10 节 Experiments）**
强制要求每条 claim 对应“实验闭环”：

- Claim → 主实验（证明有效）
- 机制证据 → 消融（证明为什么有效）
- 边界条件 → 压力测试（证明什么时候会崩）
- 公平性 → 预算/数据/训练步数对齐
- 复现性 → 固定随机种子 + 记录环境

得到项目文档（项目基准，用来当成写作资料）之后：

- **人工负责 Review**：主要看“claim-实验对齐”“baseline 是否足够强”“是否有致命假设”。
- 然后不断询问 gpt：
  - 哪些段落最容易被审稿人抓住？
  - 哪些实验设计不够严谨？
  - 哪些 claim 缺证据？
- 用这种方式迭代到文档稳定为止。

再次交给 gpt pro/gpt thinking，生成专门给 claude code 用的**执行文档**（更偏工程步骤与任务拆分）。

------

### 2.1 Claude Code

claude code 适合项目的构建阶段，可以开两三个（甚至更多）并行推进。目标：让执行文档里的任��**被真实跑通**，直到代码结构、实验入口、评测脚本都稳定。

**Claude Opus 4.5/4.6 (2025年11月发布)：**
- **SWE-bench Verified: 80.9%** - 领先业界的代码生成基准
- **200K token 上下文窗口**，64K token 输出限制
- **成本降低 67%** ($5/$25 per million tokens)
- **计算机使用增强** - 支持 zoom 动作进行详细屏幕检查
- **tau2-bench-lite: 88.9%** - Agent 工具使用评估

#### 2.1.1 Claude 使用思路（补细：角色分工 + 验证闭环）

- **核心理念：使用更好的编排，而不是更多的算力。**
  1. **并行协作：** 同时运行多个 Claude 实例（终端 5 个，浏览器 5–10 个），建议固定角色：
     - **Architect**：盯总体结构与接口
     - **Implementer**：写核心模块
     - **Tester**：写单测/跑评测/抓报错
     - **Reviewer**：做 code review + 找边界条件
       让每个实例各司其职，减少“一个模型又写又测又改”的低效循环。
       ![img](/assets/placeholder.png)
  2. **模型选择：** 反直觉地使用最大、最慢的 Opus 4.5 Thinking。
     因为它更能一次性把接口、边界、测试想清楚，减少返工。
  3. **计划先行：** 重要任务开始时先用 Plan Mode（shift+tab）：
     - 让它先输出任务拆分（TODO list）
     - 明确输入输出与验收标准（Definition of Done）
     - 再进入实现
  4. **团队记忆：** 维护 `CLAUDE.md` 并提交到 Git。建议至少包含：
     - 项目约定（命名、目录、配置格式）
     - 常见坑（数据路径、随机种子、显存爆炸原因）
     - Debug 流程（先复现→最小化→定位→修复→加测试）
  5. **自动化快捷命令：** 把重复工作流封装：`format / test / eval / report` 一键化。
     ![img](/assets/placeholder.png)
  6. **AI 互相审查：** 用 Subagents 交叉检查：
     - 一个负责“能不能跑”
     - 一个负责“是不是对”
     - 一个负责“是不是公平对比”
       ![img](/assets/placeholder.png)
  7. **集成外部工具：** 通过 MCP 让 Claude 直接查数据、看日志、定位错误（BigQuery/Sentry 等）。
  8. **【最关键的一点】建立验证循环：**
     - 每次实现必须有可验证输出：单测/脚本/最小样例
     - 每次修复必须补一个测试，防止回归
     - 没有验证闭环的"写代码"，最后一定会爆债务

#### 2.1.2 Claude Code 2.0 新特性

**核心能力升级：**

| 特性 | 说明 | 研究用途 |
|------|------|---------|
| **Sub-agents** | 专业化子Agent，并行执行 | 文献分析/代码实现/测试分离 |
| **MCP 集成** | 连接外部数据源和工具 | 访问Zotero/数据库/API |
| **Hooks** | 事件驱动的自动化 | 格式检查/测试运行/文档生成 |
| **Skills** | 可复用的能力包 | 领域特定工作流封装 |
| **Plan Mode** | 结构化任务规划 | 复杂项目的任务拆解 |
| **自定义命令** | 工作流快捷方式 | 一键运行常用操作 |

**MCP 工具生态：**
- **Zotero-MCP**: 直接访问本地文献库
- **Perplexity MCP**: 深度网络搜索
- **ClickHouse MCP**: 数据库查询
- **GitHub MCP**: 仓库操作

#### 2.1.3 最佳实践：团队协作模式

**角色分工：**

| 角色 | 职责 | Claude 实例数 |
|------|------|-------------|
| Architect | 总体结构与接口设计 | 1-2 |
| Implementer | 核心模块实现 | 2-3 |
| Tester | 测试用例与评测 | 1-2 |
| Reviewer | Code review 与边界检查 | 1-2 |

**团队记忆 (CLAUDE.md)：**
```markdown
# 项目约定
- 命名：snake_case for functions, PascalCase for classes
- 目录：遵循 cookiecutter-data-science 结构

# 常见坑
- 数据路径：使用 pathlib 而非相对路径
- 随机种子：每次实验固定 seed=42
- 显存：batch_size 先从 32 开始调试

# Debug 流程
1. 复现 bug → 2. 最小化用例 → 3. 定位 → 4. 修复 → 5. 加测试
```

### 2.2 GPT-5.2 系列

GPT-5.2 (2025年12月发布) 适合做**细节 review**：

**模型系列：**

| 模型 | 特点 | 适用场景 |
|------|------|---------|
| **GPT-5.2 Instant** | 快速响应 | 日常代码辅助、快速问答 |
| **GPT-5.2 Thinking** | 深度推理 | 复杂问题分析、算法设计 |
| **GPT-5.2 Pro** | 最强专业版 | 关键代码审查、论文评审 |

**核心能力：**
- **ARC-AGI-1: 90%+** - 首次突破 90% 通用推理基准
- **知识截止**: 2025年8月
- **长上下文**: 支持大型文档分析

**使用建议：**
- 接口一致性检查
- 实验设置公平性验证
- Silent bug 检测（数据泄漏、指标计算错误）
- PR review、评测脚本、主实验复现

### 2.3 多Agent框架

当研究任务需要多个Agent协作时，选择合适的框架：

| 框架 | 核心优势 | 研究用途 | GitHub |
|------|---------|---------|--------|
| **LangChain/LangGraph** | 复杂推理工作流，状态管理 | 多步骤文献分析、实验设计 | github.com/langchain-ai |
| **LlamaIndex** | 知识库集成，RAG 优化 | 文档密集型研究、论文库查询 | github.com/run-llama |
| **CrewAI** | 角色定义清晰，企业级 | 协作研究项目、团队模拟 | github.com/crewAIInc |
| **Microsoft AutoGen** | 对话优先框架 | 讨论式研究、辩论式分析 | github.com/microsoft/autogen |

**选择建议：**
- 文献分析 → LangChain (灵活的工作流)
- 知识库查询 → LlamaIndex (优秀的RAG)
- 团队协作 → CrewAI (清晰的角色定义)
- 对话式研究 → AutoGen (对话原语)

------

## 3. 画图

### 3.1 自动化学术插图工具

**PaperBanana** (arXiv:2601.23265, 2026)

> 《PaperBanana: Automating Academic Illustration for AI Scientists》提出了自动化学术插图生成系统，专为AI科学家设计。

**核心功能：**
- 自动识别论文中的插图需求
- 生成符合顶会风格的初版草图
- 支持多种图表类型的智能推荐

**相关工具：**
- **nanobana**: 快速图表生成
- **Diagrams**: 代码驱动的架构图生成

### 3.2 专业绘图工具对比

| 工具 | 类型 | 优势 | 适用场景 | 链接 |
|------|------|------|---------|------|
| **Mermaid.js** | 文本转图表 | 易于版本控制 | 流程图、时序图 | mermaid.js |
| **PlantUML** | UML图表 | 标准化 | 系统架构图 | plantuml.com |
| **D3.js** | 自定义可视化 | 高度定制 | 创新数据可视化 | d3js.org |
| **draw.io** | 在线绘图 | 免费、协作 | 快速原型 | diagrams.net |
| **Figma** | 协作设计 | 专业级 | 高质量插图 | figma.com |
| **PPT** | 统一绘制 | 方便整合 | 最终整合发布 | - |

### 3.3 论文图表类型清单

从几千篇论文里提取"表达强"的图，按类型建一个参考库：

1. **方法结构图 (Architecture)**
   - 整体系统架构
   - 模块关系
   - 数据流向

2. **流程图 (Pipeline)**
   - 训练流程
   - 推理流程
   - 数据处理流程

3. **训练/推理示意图**
   - 算法步骤可视化
   - 时序关系

4. **主结果表 + 消融图**
   - 对比实验表格
   - 消融研究图表

5. **失败案例可视化**
   - 边界情况
   - 错误分析

### 3.4 图表设计规范

选定视觉语言后要定规范（否则全篇风格会散）：

- **配色方案**: 颜色尽量少（论文更稳），强调用粗细/虚实区分
- **字体选择**: 统一字体族
- **线条规范**: 同一套线宽、圆角、箭头样式
- **标注规范**: 清晰的图例和说明

**GPT 辅助流程：**
1. GPT 根据论文 + 参考图生成：
   - "图的元素清单"（模块/箭头/标注）
   - "布局草案"（从左到右还是从上到下）
   - "提示词"（给绘图工具出初版草图）
2. 最后用 PPT/Figma 统一绘制与整合

------

## 4. 写作

### 4.1 个人思路（补细：写作不是“生成”，而是“论证结构”）

写作最核心的是：**让审稿人相信你的 claim**。建议按下面顺序推进：

1. **先把论文的“论证链”写出来（不写正文）**

- Claim 1 需要哪些证据？
- 证据是否来自主实验/消融/压力测试？
- 哪些证据是“必要的”，哪些只是“锦上添花”？

1. **再写 Outline（每一节只写要点）**

- 每小节回答一个问题：
  - 这节要说服审稿人什么？
  - 审稿人可能质疑什么？
  - 我用什么证据挡住？

1. **最后再让模型生成具体文字**

- 这样生成的文字不会空，不会“像论文但没信息”。

#### 4.1.1 MCP 工具与 Skills

**MCP工具（用于文献搜索与访问）：**

| 工具 | 功能 | 适用场景 |
|------|------|---------|
| **Zotero-MCP** | 直接访问本地Zotero文献库 | 文献管理、引用分析 |
| **Perplexity MCP** | Deep Research模式 | 技术调研、综述写作 |
| **ClickHouse MCP** | 数据库集成 | 大数据分析 |
| **Zen MCP** | 研究工作流 | 流程自动化 |
| **Paper Search MCP** | 多数据库搜索 | 快速获取新论文 |

**Skills工具（用于科研分析与写作）：**

| 工具 | 技能数量 | 特色 |
|------|---------|------|
| **claude-scientific-skills** | 140+ | 即用型技能，多领域覆盖 |
| **claude-scientific-writer** | - | 专注科研写作 |
| **Content-Research-Writer** | - | 长文协作写作 |
| **20-ml-paper-writing** | 77项AI科研技能 | 完整ML论文写作流程 |

**GitHub 资源：**
- claude-scientific-skills: https://github.com/K-Dense-AI/claude-scientific-skills

#### 4.1.2 多模型协作写作

**tmux 工作流配置：**

让 Claude / GPT-5.2 / Gemini Pro 分工协作：

| 实例 | 负责内容 | 协作方式 |
|------|---------|---------|
| Instance 1 | Related Work | 文献梳理与对比 |
| Instance 2 | Method 写作 | 方法描述与形式化 |
| Instance 3 | 实验描述与表格 | 实验设计和结果 |
| Instance 4 | 一致性校对 | 术语、符号、变量统一 |

**关键原则：**
- 同时进行，而非顺序
- 定期同步，避免冲突
- 共享术语表，确保一致性

#### 4.1.3 AI-research-SKILLs

"20-ml-paper-writing"模块制定了严格规则：**绝不凭记忆生成引用**，强制 5 步：检索 → 双源验证 → 获取官方 BibTeX → 核对摘要 → 写入。

覆盖 AI 科研全流程的 77 项技能（20 个技术板块）。

### 4.2 Prism (OpenAI, 2026)

Prism 是 OpenAI 推出的 LaTeX 原生科研写作工作区，**发布于 2026年1月27日**。

**核心特性：**
- **LaTeX 原生工作区** - 无缝迁移现有项目
- **GPT-5.2 集成** - 强大的 AI 写作辅助
- **实时协作** - 多人同时编辑
- **中英文支持** - 双语写作无障碍
- **公式和图表创建** - 内置可视化工具
- **引用管理** - 智能文献引用

**对比 Overleaf：**
| 特性 | Prism | Overleaf |
|------|-------|----------|
| AI 集成 | GPT-5.2 原生 | 需外部插件 |
| 实时协作 | ✓ | ✓ |
| 中文支持 | ✓ | 需配置 |
| 价格 | 免费（个人账户） | 有限制 |

**官网**: https://openai.com/prism/

### 4.3 写作最佳实践

#### 论证链构建

写作的核心是**让审稿人相信你的 claim**：

1. **先把论文的"论证链"写出来（不写正文）**
   - Claim 1 需要哪些证据？
   - 证据是否来自主实验/消融/压力测试？
   - 哪些证据是"必要的"，哪些只是"锦上添花"？

2. **再写 Outline（每一节只写要点）**
   - 每小节回答一个问题
   - 这节要说服审稿人什么？
   - 审稿人可能质疑什么？
   - 我用什么证据挡住？

3. **最后再让模型生成具体文字**
   - 这样生成的文字不会空，不会"像论文但没信息"

#### 会议特定建议

| 会议 | 偏好 | 写作建议 |
|------|------|---------|
| **NeurIPS** | 神经科学结合，跨学科创新 | 强调方法的生物学合理性 |
| **ICML** | 理论基础，数学严谨 | 增加理论分析和证明 |
| **ICLR** | 表示学习，深入理解 | 侧重表示分析和可视化 |
| **CVPR** | 实验完整性，SOTA对比 | 充分的消融和对比实验 |
| **ACL/EMNLP** | 语言学洞察，人类评估 | 增加定性分析和案例分析 |

#### 常见写作误区

- ❌ "像论文但没信息" - 缺乏实质贡献
- ❌ 过度声明 - claim 超出实验支持
- ❌ 忽略关键 baseline - 对比不充分
- ❌ 叙事不清 - 逻辑跳跃

------

## 5. 审稿（补细：从"让它挑刺"到"迭代闭环"）

### 5.1 结构化审稿流水线

把审稿输出变成可执行修改任务：

**审稿输出要结构化：**
- 哪条 claim 缺证据？
- 哪个实验最关键但没做？
- 哪个 baseline 必须补？
- 哪段叙事最像"空话"？
- 哪个地方最容易被说不新？

**把审稿意见变成 TODO 列表：**
- 每条意见对应一个"改动 + 验收标准"：
  - "补消融：去掉模块 X → 指标下降 ≥ 某阈值"
  - "补公平对比：训练步数对齐 + 算力对齐"
  - "补压力测试：低资源/噪声/分布外"

**循环迭代（10+ 次）：**
- 每轮只解决最致命的 3–5 条，别贪多
- 使用 Claude + GPT-5.2 + Gemini Pro 并行审稿
- 交叉验证审稿意见，找出真正的问题

### 5.2 会议特定审稿模板

不同会议有不同的审稿标准，以下是定制化模板要点：

| 会议 | 审稿重点 | 常见质疑 |
|------|---------|---------|
| **NeurIPS** | 理论创新 + 实验验证 | "神经科学合理性？""SOTA 对比？" |
| **ICML** | 数学严谨 + 理论分析 | "证明是否完整？""设定是否合理？" |
| **ICLR** | 表示理解 + 可视化 | "真正学到了什么？""可视化？" |
| **CVPR** | 实验完整性 | "消融是否充分？""数据集是否公平？" |
| **IROS** | 系统实现 + 真实场景 | "硬件实现？""真实机器人验证？" |

### 5.3 Rebuttal 写作

**Rebuttal 策略：**

1. **分类回应**
   - **可修复的**: 明确承诺修改，给出具体方案
   - **误解的**: 礼貌澄清，补充说明
   - **不同意但合理的**: 讨论并引用支持文献
   - **不同意的**: 坚定但有理有据地反驳

2. **常见意见应对模板**
   ```
   # 缺少消融实验
   "感谢审稿人的建议。我们已在补充材料中添加了消融实验[表X]，
   显示去掉模块A后性能下降Y%，证实了其必要性。"

   # 对比不公平
   "我们已确保所有方法使用相同的训练设置（学习率、batch size等），
   详见补充材料[实验设置]。"

   # 创新性不足
   "与[ABC方法]不同，我们关注的是X场景下的Y问题，这在之前
   工作中未被探索。我们的贡献在于Z。"
   ```

3. **成功案��要素**
   - 感谢审稿人（即使意见尖锐）
   - 具体修改措施（不是泛泛而谈）
   - 实验数据支持（新增结果）
   - 保持专业和礼貌

------

## 6. 工具生态总览

### 6.1 工具选择矩阵

| 任务阶段 | 推荐工具 | 备选方案 | 选择理由 |
|---------|---------|---------|---------|
| **文献检索** | Elicit | Consensus, Semantic Scholar | 1.25亿+论文，自动化综述 |
| **文献管理** | Zotero-MCP | Mendeley, Papers | Claude 集成，语义搜索 |
| **代码生成** | Claude Code | GPT-5.2, Codex | 200K上下文，80.9% SWE-bench |
| **Agent编排** | LangChain/LangGraph | CrewAI, AutoGen | 灵活工作流，强大社区 |
| **论文写作** | Prism | Overleaf | GPT-5.2 集成，LaTeX 原生 |
| **审稿模拟** | Claude + GPT-5.2 | Gemini Pro | 多模型交叉验证 |
| **画图** | PaperBanana + Mermaid | D3.js, Figma | 自动化 + 版本控制 |

### 6.2 完整研究工作流

从 idea 到论文发表的典型时间线（10-14周）：

```
Week 1-2:  调研阶段
├── Elicit 快速扫描领域
├── Sci-Reasoning 分析创新模式
└── 产出 Research Gap 清单

Week 3:    项目设计
├── Claude Code 生成项目文档
├── GPT-5.2 审查 claim-实验对齐
└── 确定实验计划

Week 4-7:  代码实现 + 实验
├── Claude Code + Sub-agents 并行开发
├── 自动化 ablation/stress test
└── 实验结果自动汇总

Week 8-9:  论文写作
├── Prism + GPT-5.2 生成初稿
├── 多模型协作完善各章节
└── PaperBanana 生成图表

Week 10:   审稿迭代
├── 10+ 轮自动审稿
├── TODO 驱动的修改
└── 人工最终审查

Week 11:   投稿
├── 格式检查
├── 补充材料准备
└── 提交

Week 12+:  Rebuttal (如需要)
├── 分析审稿意见
├── 分类回应
└── 补充实验
```

### 6.3 快速参考

**常用命令速查：**
```bash
# Claude Code
claude code --plan      # 启动 plan mode
claude code --hook      # 查看可用 hooks

# MCP
mcp list               # 列出已安装的 MCP 服务器
mcp install zotero     # 安装 Zotero-MCP

# 文献检索
elicit search "topic"  # Elicit 搜索
```

**关键链接汇总：**
- Sci-Reasoning: https://github.com/AmberLJC/Sci-Reasoning
- The AI Scientist: https://github.com/SakanaAI/AI-Scientist
- Elicit: https://elicit.com
- Prism: https://openai.com/prism/
- claude-scientific-skills: https://github.com/K-Dense-AI/claude-scientific-skills

------

> **最后提醒**: 工具是辅助，核心是你的研究洞察力。让 AI 帮你处理重复性工作，把时间留给真正的思考。

调用 claude / gpt 5.2 / gemini3 pro 的 API，写好 prompt（不同论文不同 prompt）：

```python
"""You are a world-leading expert in the field of assistive technology, computer vision for robotics, and edge computing.

The following text is a LaTeX draft of a paper intended for submission to IROS (IEEE/RSJ International Conference on Intelligent Robots and Systems), a top-tier robotics conference.

Please review this manuscript with the MOST CRITICAL, HARSH, STRICT, and MEAN perspective possible - like a picky IROS reviewer who rejects most papers.

Focus your review on:
1. Technical soundness and novelty
2. Clarity of the contribution and presentation
3. Experimental evaluation methodology
4. Comparison with prior art (especially edge-based assistive systems)
5. Writing quality and logical flow

CRITICAL INSTRUCTIONS:
- Be extremely critical - point out EVERY weakness
- Question claims that lack proper evidence
- Highlight vague or hand-wavy descriptions
- Point out missing baselines or ablation studies
- Criticize overstatements or exaggerated claims
- Check if the contribution is truly significant enough for IROS

Please write your review in the following format:

---
Overall Merit: (4-point scale: 1=Accept, 2=Weak Accept, 3=Weak Reject, 4=Reject)

Paper Summary: (1-2 paragraphs summarizing what the paper claims to do)

Strengths: (2-3 bullet points - be sparing, only genuine strengths)

Weaknesses: (4-6 bullet points - be thorough and harsh)
- [Specific technical issue]
- [Missing experimental validation]
- [Unclear writing or logic gap]
- [Overstated claim]
- ...

Subjective Evaluation:
(Detailed assessment covering:
- Is the contribution significant enough for IROS?
- Are the experiments sufficient? What's missing?
- Is the comparison with prior work fair and complete?
- Are the claims properly supported by evidence?
- Specific questions for the authors to address
- Recommended improvements if revision were possible)

---

PAPER CONTENT (LaTeX format):
"""

review_prompt += paper_content
```

- 得到审稿结果之后，可以写个循环，迭代 10 余次，然后论文大致结果就会非常稳定。
````
